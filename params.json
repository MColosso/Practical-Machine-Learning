{"name":"Practical-machine-learning","tagline":"Coursera - Johns Hopkins Specialization in Data Science - Practical Machine Learning - Peer Assesments","body":"\r\n## Human Activity Recognition\r\n\r\n### Introduction\r\n\r\nUsing devices such as _Jawbone Up_, _Nike FuelBand_, and _Fitbit_ it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement - a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: [http://groupware.les.inf.puc-rio.br/har](http://groupware.les.inf.puc-rio.br/har)\r\n\r\n### Objective\r\n\r\nDevelop a machine learning algorithm with the variables content in the training dataset ('pml-training') and how they interact to get the result (\"classe\"), and apply this algorithm to each of the 20 test cases in the testing dataset ('pml_testing')\r\n\r\n### Data\r\n\r\nThe data for this project come from this source: [http://groupware.les.inf.puc-rio.br/har](http://groupware.les.inf.puc-rio.br/har).\r\n\r\nThe training data for this project are available here: [pml-training.csv](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv), and the test data are available here: [pml-testing.csv](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv)\r\n\r\n\r\n```r\r\n  library(caret)\r\n```\r\n\r\n```\r\n## Loading required package: lattice\r\n## Loading required package: ggplot2\r\n```\r\n\r\n```r\r\n  set.seed(1234)\r\n\r\n#  download.file(url=\"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv\",\r\n#                destfile=\"pml-training.csv\", method=\"internal\")\r\n#  download.file(url=\"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv\",\r\n#                destfile=\"pml-testing.csv\", method=\"internal\")\r\n  pml_training <- read.csv(\"pml-training.csv\",\r\n                       na.strings=c(\"\", \"NA\", \"#DIV/0!\"),\r\n                       stringsAsFactors=TRUE)\r\n  pml_testing  <- read.csv(\"pml-testing.csv\",\r\n                       na.strings=c(\"\", \"NA\", \"#DIV/0!\"),\r\n                       stringsAsFactors=TRUE)\r\n```\r\n\r\n#### Data analysis\r\n\r\nFirst, we divided the training data ('pml-training.csv') in two chunks: training (60% of data) and testing (remaining 40%) for further testing of selected model.\r\n\r\n\r\n```r\r\n  inTrain <- createDataPartition(y=pml_training$classe,\r\n                                 p=0.60, list=FALSE)\r\n  training <- pml_training[ inTrain, ]\r\n  testing  <- pml_training[-inTrain, ]\r\n\r\n  table(testing$classe)\r\n```\r\n\r\n```\r\n## \r\n##    A    B    C    D    E \r\n## 2232 1518 1368 1286 1442\r\n```\r\n\r\n```r\r\n#  head(training)\r\n#  summary(training)\r\n```\r\n\r\nThere are 100 variables (of 160) with hign number of missing values (11540+ of 11776 records = 98%)\r\n\r\n\r\n```r\r\n  NAs <- vector(length=ncol(training))\r\n  for(i in 1:ncol(training)) NAs[i]<- sum(is.na(training[, i]))\r\n  table(factor(NAs))\r\n```\r\n\r\n```\r\n## \r\n##     0 11535 11537 11541 11542 11560 11582 11583 11584 11776 \r\n##    60    69     5     2     5     2     2     7     2     6\r\n```\r\n\r\n```r\r\n  dim(training)\r\n```\r\n\r\n```\r\n## [1] 11776   160\r\n```\r\n\r\nSince these variables do not influence the result, we will remove them along with some invariants ('new_window') and descriptive variables ('user_name', for example)\r\n\r\n\r\n```r\r\n# Remove columns with NAs\r\n  training <- training[,!sapply(training,function(x) any(is.na(x)))]\r\n  dim(training)\r\n```\r\n\r\n```\r\n## [1] 11776    60\r\n```\r\n\r\n```r\r\n  nZvar <- nearZeroVar(training, saveMetrics=TRUE)\r\n\r\n  # After removing columns with NAs, there are one with near-zero variance:\r\n  head(nZvar[order(nZvar$nzv, decreasing=TRUE), ], n=5L)\r\n```\r\n\r\n```\r\n##                      freqRatio percentUnique zeroVar   nzv\r\n## new_window           47.863071    0.01698370   FALSE  TRUE\r\n## X                     1.000000  100.00000000   FALSE FALSE\r\n## user_name             1.066543    0.05095109   FALSE FALSE\r\n## raw_timestamp_part_1  1.035714    7.09918478   FALSE FALSE\r\n## raw_timestamp_part_2  1.000000   90.71841033   FALSE FALSE\r\n```\r\n\r\n```r\r\n  # There are columns that, by their nature, do not influence the resulting value:\r\n  # raw_timestamp_part_1  raw_timestamp_part_2  cvtd_timestamp\r\n\r\n  # Other variables are not related with the resulting:\r\n  # X                     user_name             new_window            num_window\r\n\r\n  # Remove this columns\r\n  training <- training[ , -c(1:7)]   # This variables correspond to the first\r\n                                     # 7 variables of the dataframe\r\n```\r\n\r\n#### Model Train\r\n\r\nWe selected Breiman and Cutler's Random Forest algorithm for prediction due to the accuracy of results.\r\n\r\nFirst, we used the implementation of Random Forest in `train()` function:\r\n\r\n```\r\n  modelFit <- train(classe ~ ., data=training, method=\"rf\")\r\n  pred <- predict(modelFit, testing)\r\n  table(pred, testing$classe)\r\n  \r\n# pred    A    B    C    D    E\r\n#    A 2231   14    0    0    0\r\n#    B    1 1499    7    1    0\r\n#    C    0    4 1352   10    4\r\n#    D    0    1    9 1274    2\r\n#    E    0    0    0    1 1436\r\n\r\n```\r\n\r\nbut the processing speed was a real con (more than 4 hours and a half). Finally, we used the `randomForest()` function of randomForest library:\r\n\r\n\r\n```r\r\n  library(randomForest)\r\n\r\n  modelFit <- randomForest(classe ~ ., data=training, importance=TRUE)\r\n  modelFit\r\n```\r\n\r\n```\r\n## \r\n## Call:\r\n##  randomForest(formula = classe ~ ., data = training, importance = TRUE) \r\n##                Type of random forest: classification\r\n##                      Number of trees: 500\r\n## No. of variables tried at each split: 7\r\n## \r\n##         OOB estimate of  error rate: 0.59%\r\n## Confusion matrix:\r\n##      A    B    C    D    E class.error\r\n## A 3342    5    0    0    1 0.001792115\r\n## B   10 2266    3    0    0 0.005704256\r\n## C    0   19 2031    4    0 0.011197663\r\n## D    0    0   20 1909    1 0.010880829\r\n## E    0    0    1    6 2158 0.003233256\r\n```\r\n\r\n```r\r\n  varImpPlot(modelFit)   # Show more important variables\r\n```\r\n\r\n![](https://raw.githubusercontent.com/MColosso/Practical-Machine-Learning/master/Human_Activity_Recognition_files/figure-html/Model_train-1.png) \r\n\r\nwith a gain in speed and accuracy.\r\n\r\n#### Model Validation\r\n\r\n\r\n```r\r\n  pred <- predict(modelFit, testing)\r\n  table(pred, testing$classe)\r\n```\r\n\r\n```\r\n##     \r\n## pred    A    B    C    D    E\r\n##    A 2229   12    0    0    0\r\n##    B    3 1503   14    0    0\r\n##    C    0    3 1350   23    1\r\n##    D    0    0    4 1260    5\r\n##    E    0    0    0    3 1436\r\n```\r\n\r\n```r\r\n  print(confusionMatrix(pred, testing$classe))\r\n```\r\n\r\n```\r\n## Confusion Matrix and Statistics\r\n## \r\n##           Reference\r\n## Prediction    A    B    C    D    E\r\n##          A 2229   12    0    0    0\r\n##          B    3 1503   14    0    0\r\n##          C    0    3 1350   23    1\r\n##          D    0    0    4 1260    5\r\n##          E    0    0    0    3 1436\r\n## \r\n## Overall Statistics\r\n##                                          \r\n##                Accuracy : 0.9913         \r\n##                  95% CI : (0.989, 0.9933)\r\n##     No Information Rate : 0.2845         \r\n##     P-Value [Acc > NIR] : < 2.2e-16      \r\n##                                          \r\n##                   Kappa : 0.989          \r\n##  Mcnemar's Test P-Value : NA             \r\n## \r\n## Statistics by Class:\r\n## \r\n##                      Class: A Class: B Class: C Class: D Class: E\r\n## Sensitivity            0.9987   0.9901   0.9868   0.9798   0.9958\r\n## Specificity            0.9979   0.9973   0.9958   0.9986   0.9995\r\n## Pos Pred Value         0.9946   0.9888   0.9804   0.9929   0.9979\r\n## Neg Pred Value         0.9995   0.9976   0.9972   0.9960   0.9991\r\n## Prevalence             0.2845   0.1935   0.1744   0.1639   0.1838\r\n## Detection Rate         0.2841   0.1916   0.1721   0.1606   0.1830\r\n## Detection Prevalence   0.2856   0.1937   0.1755   0.1617   0.1834\r\n## Balanced Accuracy      0.9983   0.9937   0.9913   0.9892   0.9977\r\n```\r\n\r\nResults show a 99.3% of accuracy in our test dataset.\r\n\r\n\r\n### Test Set Prediction\r\n\r\nApply the machine learning algorithm built to each of the 20 test cases in the testing data set ('pml_testing'):\r\n\r\n\r\n```r\r\n  answers <- predict(modelFit, pml_testing)\r\n  answers\r\n```\r\n\r\n```\r\n##  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 \r\n##  B  A  B  A  A  E  D  B  A  A  B  C  B  A  E  E  A  B  B  B \r\n## Levels: A B C D E\r\n```\r\n\r\nand generate the answer files\r\n\r\n\r\n```r\r\n  pml_write_files = function(x){\r\n    n = length(x)\r\n    for(i in 1:n){\r\n      filename = paste0(\"problem_id_\",i,\".txt\")\r\n      write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)\r\n    }\r\n  }\r\n  \r\n  pml_write_files(as.character(answers))\r\n```\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}