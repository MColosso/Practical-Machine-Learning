---
output:
  html_document:
    keep_md: yes
---

## Human Activity Recognition

### Introduction

Using devices such as _Jawbone Up_, _Nike FuelBand_, and _Fitbit_ it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement - a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: [http://groupware.les.inf.puc-rio.br/har](http://groupware.les.inf.puc-rio.br/har)

### Objective

Develop a machine learning algorithm with the variables content in the training dataset ('pml-training') and how they interact to get the result ("classe"), and apply this algorithm to each of the 20 test cases in the testing dataset ('pml_testing')

### Data

The data for this project come from this source: [http://groupware.les.inf.puc-rio.br/har](http://groupware.les.inf.puc-rio.br/har).

The training data for this project are available here: [pml-training.csv](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv), and the test data are available here: [pml-testing.csv](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv)

```{r Load_data, cache=TRUE}

  library(caret)
  set.seed(1234)

#  download.file(url="https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv",
#                destfile="pml-training.csv", method="internal")
#  download.file(url="https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv",
#                destfile="pml-testing.csv", method="internal")
  pml_training <- read.csv("pml-training.csv",
                       na.strings=c("", "NA", "#DIV/0!"),
                       stringsAsFactors=TRUE)
  pml_testing  <- read.csv("pml-testing.csv",
                       na.strings=c("", "NA", "#DIV/0!"),
                       stringsAsFactors=TRUE)
```

#### Data analysis

First, we divided the training data ('pml-training.csv') in two chunks: training (60% of data) and testing (remaining 40%) for further testing of selected model.

```{r Data_analysis, cache=TRUE}

  inTrain <- createDataPartition(y=pml_training$classe,
                                 p=0.60, list=FALSE)
  training <- pml_training[ inTrain, ]
  testing  <- pml_training[-inTrain, ]

  table(testing$classe)

#  head(training)
#  summary(training)

```

There are 100 variables (of 160) with hign number of missing values (11540+ of 11776 records = 98%)

```{r Data_analysis2, cache=TRUE}

  NAs <- vector(length=ncol(training))
  for(i in 1:ncol(training)) NAs[i]<- sum(is.na(training[, i]))
  table(factor(NAs))
  dim(training)

```

Since these variables do not influence the result, we will remove them along with some invariants ('new_window') and descriptive variables ('user_name', for example)

```{r Data_analysis3, cache=TRUE}

# Remove columns with NAs
  training <- training[,!sapply(training,function(x) any(is.na(x)))]
  dim(training)

  nZvar <- nearZeroVar(training, saveMetrics=TRUE)

  # After removing columns with NAs, there are one with near-zero variance:
  head(nZvar[order(nZvar$nzv, decreasing=TRUE), ], n=5L)

  # There are columns that, by their nature, do not influence the resulting value:
  # raw_timestamp_part_1  raw_timestamp_part_2  cvtd_timestamp

  # Other variables are not related with the resulting:
  # X                     user_name             new_window            num_window

  # Remove this columns
  training <- training[ , -c(1:7)]   # This variables correspond to the first
                                     # 7 variables of the dataframe

```

#### Model Train

We selected Breiman and Cutler's Random Forest algorithm for prediction due to the accuracy of results.

First, we used the implementation of Random Forest in `train()` function:

```
  modelFit <- train(classe ~ ., data=training, method="rf")
  pred <- predict(modelFit, testing)
  table(pred, testing$classe)
  
# pred    A    B    C    D    E
#    A 2231   14    0    0    0
#    B    1 1499    7    1    0
#    C    0    4 1352   10    4
#    D    0    1    9 1274    2
#    E    0    0    0    1 1436

```

but the processing speed was a real con (more than 4 hours and a half). Finally, we used the `randomForest()` function of randomForest library:

```{r Model_train, cache=TRUE}

  library(randomForest)

  modelFit <- randomForest(classe ~ ., data=training, importance=TRUE)
  modelFit

  varImpPlot(modelFit)   # Show more important variables

```

with a gain in speed and accuracy.

#### Model Validation

```{r Model_validation, cache=TRUE}

  pred <- predict(modelFit, testing)
  table(pred, testing$classe)

  print(confusionMatrix(pred, testing$classe))

```

Results show a 99.3% of accuracy in our test dataset.


### Test Set Prediction

Apply the machine learning algorithm built to each of the 20 test cases in the testing data set ('pml_testing'):

```{r}

  answers <- predict(modelFit, pml_testing)
  answers

```

and generate the answer files

```{r Answer_files, cache=TRUE}

  pml_write_files = function(x){
    n = length(x)
    for(i in 1:n){
      filename = paste0("problem_id_",i,".txt")
      write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
    }
  }
  
  pml_write_files(as.character(answers))

```
